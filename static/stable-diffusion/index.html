<html>
<!--
    Usage:
        http://localhost:8000/sd-demo.html // defaults to WebNN GPU
        http://localhost:8000/sd-demo.html?provider=webnn&device=gpu
        http://localhost:8000/sd-demo.html?provider=webnn&device=cpu
        http://localhost:8000/sd-demo.html?provider=wasm

        provider = wasm | webnn | webgpu | webgl
        device = cpu | gpu | npu // applicable to WebNN
-->

<head>
    <meta charset='utf-8'>
    <title>WebNN Stable Diffusion</title>
    <link rel="stylesheet" href="main.css">
</head>

<body>
    <div class="grid-2 p1 v1">
        <h1 class="title"><span id="title">WebNN</span> Stable Diffusion
        </h1>
        <p id="error"></p>
    </div>
    <div class="container v9">
        <div class="left">
            <div class="input-group">
                <textarea placeholder="Enter your prompt here" rows="2" type="text"
                    id="positive_prompt">giant castle, mountains, sunrise, volumetric lighting</textarea>
                <div class="tokeninfo" id="positive_token_info">
                    63/75
                </div>
            </div>
            <div class="input-group">
                <input type="text" id="negative_prompt" placeholder="negative prompt" />
                <div class="tokeninfo negative" id="negative_token_info">
                    75/75
                </div>
            </div>
            <div class="input-group grid-4 options">
                <div>
                    <div>
                        <label for="prompt">Seed</label>
                    </div>
                    <div class="grid-4-41 seed">
                        <input type="text" id="userSeed" value="123465" maxlength="6" minlength="6"></input>
                        <button id="changeSeed" value="">
                            <svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 -960 960 960" width="24">
                                <path
                                    d="M204-318q-22-38-33-78t-11-82q0-134 93-228t227-94h7l-64-64 56-56 160 160-160 160-56-56 64-64h-7q-100 0-170 70.5T240-478q0 26 6 51t18 49l-60 60ZM481-40 321-200l160-160 56 56-64 64h7q100 0 170-70.5T720-482q0-26-6-51t-18-49l60-60q22 38 33 78t11 82q0 134-93 228t-227 94h-7l64 64-56 56Z" />
                            </svg>
                        </button>
                    </div>
                </div>
                <div></div>
                <div></div>
            </div>
            <div class="button-group key">
                <button id="load_models" class="button">
                    Load Models
                </button>
                <button id="generate_next_image" disabled>
                    <svg xmlns="http://www.w3.org/2000/svg" width="128" height="128" viewBox="0 0 256 256">
                        <path fill="#ffffff"
                            d="m197.58 129.06l-51.61-19l-19-51.65a15.92 15.92 0 0 0-29.88 0L78.07 110l-51.65 19a15.92 15.92 0 0 0 0 29.88L78 178l19 51.62a15.92 15.92 0 0 0 29.88 0l19-51.61l51.65-19a15.92 15.92 0 0 0 0-29.88ZM140.39 163a15.87 15.87 0 0 0-9.43 9.43l-19 51.46L93 172.39a15.87 15.87 0 0 0-9.39-9.39l-51.46-19l51.46-19a15.87 15.87 0 0 0 9.39-9.39l19-51.46l19 51.46a15.87 15.87 0 0 0 9.43 9.43l51.46 19ZM144 40a8 8 0 0 1 8-8h16V16a8 8 0 0 1 16 0v16h16a8 8 0 0 1 0 16h-16v16a8 8 0 0 1-16 0V48h-16a8 8 0 0 1-8-8m104 48a8 8 0 0 1-8 8h-8v8a8 8 0 0 1-16 0v-8h-8a8 8 0 0 1 0-16h8v-8a8 8 0 0 1 16 0v8h8a8 8 0 0 1 8 8" />
                    </svg>
                    Generate Image
                </button>
            </div>
            <div class="grid-2">
                <div>
                    <div class="progress-bar">
                        <div id="progress-bar-inner" class="progress-bar-inner"></div>
                    </div>
                    <div class="progress-bar-label" id="progress-bar-label">0%</div>
                </div>
                <div>
                    <div class="progress-bar">
                        <div id="progress-bar-inner-inference" class="progress-bar-inner"></div>
                    </div>
                    <div class="progress-bar-label" id="progress-bar-label-inference">0%</div>
                </div>
            </div>

            <div class="log-output" id='status'>
            </div>
            <table id="data" class="hide">
                <tr>
                    <th></th>
                    <th class="load">Load Time</th>
                    <th class="create">Create Time</th>
                    <th class="run">Run Time (Inference)</th>
                </tr>
                <tr id="textencoder">
                    <td>Text Encoder</td>
                    <td id="textencoderload"></td>
                    <td id="textencodercreate"></td>
                    <td id="textencoderrun"></td>
                </tr>
                <tr id="unet">
                    <td>UNet</td>
                    <td id="unetload"></td>
                    <td id="unetcreate"></td>
                    <td id="unetrun"></td>
                </tr>
                <tr id="vaedecode">
                    <td>VAE Decode</td>
                    <td id="vaedecodeload"></td>
                    <td id="vaedecodecreate"></td>
                    <td id="vaedecoderun"></td>
                </tr>
                <tr id="total">
                    <td>Total</td>
                    <td id="totalload"></td>
                    <td id="totalcreate">N/A</td>
                    <td id="totalrun"></td>
                </tr>
            </table>
            <div id="webnnstatus"><span id="circle"></span> <span id="info">WebNN</span></div>
        </div>
        <div class="right">
            <canvas class='canvas' id='canvas' width='512' height='512'>
                Canvas is not supported. You'll need to try a newer browser version or another browser.
            </canvas>

        </div>
    </div>
</body>

<script src='../ort/ort.all.min.js'></script>
<script type='module'>
    import * as Utils from './utils.js';

    // Configuration...
    const pixelWidth = 512;
    const pixelHeight = 512;
    const latentWidth = (pixelWidth / 8);
    const latentHeight = (pixelHeight / 8);
    const latentChannelCount = 4;
    const unetBatch = 2;
    const unetChannelCount = 4;
    const textEmbeddingSequenceLength = 77;
    const textEmbeddingSequenceWidth = 768;
    const unetIterationCount = 25; // Hard-coded number of samples, since the denoising weight ramp is constant.
    let seed = BigInt(123465);
    let performanceData = {
        loadtime: {
            textencoder: 0,
            unet: 0,
            vaedecode: 0,
            total: 0
        },
        sessioncreate: {
            textencoder: 0,
            unet: 0,
            vaedecode: 0,
            total: 0
        },
        sessionrun: {
            textencoder: 0,
            unet: [],
            unettotal: 0,
            vaedecode: 0,
            total: 0
        }
    };

    Utils.log('Loading ONNX Runtime...');
    const progressBarInner = document.getElementById("progress-bar-inner");
    const progressBarLabel = document.getElementById("progress-bar-label");
    const progressBarInnerInference = document.querySelector("#progress-bar-inner-inference");
    const progressBarLabelInference = document.querySelector("#progress-bar-label-inference");
    const startButton = document.getElementById('generate_next_image');
    const loadButton = document.getElementById('load_models');
    const logOutput = document.getElementById("status");
    const positiveInput = document.getElementById('positive_prompt');
    const negativeInput = document.getElementById('negative_prompt');
    const positiveTokenInfo = document.getElementById('positive_token_info');
    const negativeTokenInfo = document.getElementById('negative_token_info');
    const error = document.querySelector('#error');
    const userSeed = document.querySelector('#userSeed');
    const changeSeed = document.querySelector('#changeSeed');
    const title = document.querySelector('#title');
    const data = document.querySelector('#data');
    const textencoderload = document.querySelector('#textencoderload');
    const textencodercreate = document.querySelector('#textencodercreate');
    const textencoderrun = document.querySelector('#textencoderrun');
    const unetload = document.querySelector('#unetload');
    const unetcreate = document.querySelector('#unetcreate');
    const unetrun = document.querySelector('#unetrun');
    const vaedecodeload = document.querySelector('#vaedecodeload');
    const vaedecodecreate = document.querySelector('#vaedecodecreate');
    const vaedecoderun = document.querySelector('#vaedecoderun');
    const totalload = document.querySelector('#totalload');
    const totalcreate = document.querySelector('#totalcreate');
    const totalrun = document.querySelector('#totalrun');
    let progress = 0;
    let inferenceProgress = 0;

    logOutput.addEventListener("DOMSubtreeModified", () => {
        logOutput.scrollTop = logOutput.scrollHeight;
    });

    loadButton.onclick = async () => {
        loadButton.disabled = true;
        startButton.disabled = true;
        await loadStableDiffusion(executionProvider, progress);
        progress = 0;
        startButton.disabled = false;
    }

    startButton.onclick = async () => {
        startButton.disabled = true;
        await generateNextImage();
        inferenceProgress = 0;
    }

    positiveInput.addEventListener('input', async (e) => {
        const inputValue = e.target.value;
        const ids = await Utils.getTokenizers(inputValue);
        // Max token length is 75.
        const left_tokens_length = 75 - ids.length;
        positiveTokenInfo.innerHTML = `${(left_tokens_length <= 0) ? 0 : left_tokens_length}/75 tokens left`;
    });

    negativeInput.addEventListener('input', async (e) => {
        const inputValue = e.target.value;
        const ids = await Utils.getTokenizers(inputValue);
        // Max token length is 75.
        const left_tokens_length = 75 - ids.length;
        negativeTokenInfo.innerHTML = `${(left_tokens_length <= 0) ? 0 : left_tokens_length}/75 tokens left`;
    });

    async function getTextTokens() {
        const positiveText = positiveInput.value;
        const negativeText = negativeInput.value;

        // A string like 'a cute magical flying ghost dog, fantasy art, golden color, high quality, highly detailed, elegant, sharp focus, concept art, character concepts, digital painting, mystery, adventure'
        // becomes a 1D tensor of {49406, 320, 2242, 7823, 4610, 7108, 1929, 267, 5267, 794, 267, 3878, 3140, 267, 1400, 3027, ...}
        // padded with blanks (id 49407) up to the maximum sequence length of the text encoder (typically 77).
        // So the text encoder can't really handle more than 75 words (+1 start, +1 stop token),
        // not without some extra tricks anyway like calling it multiple times and combining the embeddings.
        let positive_token_ids = [49406]; // Inits with start token
        let negative_token_ids = [49406];
        const positive_text_ids = await Utils.getTokenizers(positiveText);
        positive_token_ids = positive_token_ids.concat(positive_text_ids);
        if (positive_text_ids.length > (textEmbeddingSequenceLength - 2)) { // Max inputs ids should be 75
            positive_token_ids = positive_token_ids.slice(0, (textEmbeddingSequenceLength - 1));
            positive_token_ids.push(49407);
        } else {
            const fillerArray = new Array(textEmbeddingSequenceLength - positive_token_ids.length).fill(49407);
            positive_token_ids = positive_token_ids.concat(fillerArray);
        }

        let negative_text_ids = await Utils.getTokenizers(negativeText);
        negative_token_ids = negative_token_ids.concat(negative_text_ids);
        if (negative_text_ids.length > (textEmbeddingSequenceLength - 2)) {
            negative_token_ids = negative_token_ids.slice(0, (textEmbeddingSequenceLength - 1));
            negative_token_ids.push(49407);
        } else {
            const fillerArray = new Array(textEmbeddingSequenceLength - negative_token_ids.length).fill(49407);
            negative_token_ids = negative_token_ids.concat(fillerArray);
        }

        const token_ids = positive_token_ids.concat(negative_token_ids);
        return token_ids;
    };

    Utils.log('...loaded');

    function convertPlanarFloat16RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            rgba[j + 0] = (Utils.decodeFloat16(input[redInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 1] = (Utils.decodeFloat16(input[greenInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 2] = (Utils.decodeFloat16(input[blueInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    function convertPlanarUint8RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            let inputValue = input[redInputOffset + i];
            rgba[j + 0] = inputValue;
            rgba[j + 1] = inputValue;
            rgba[j + 2] = inputValue;
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    function convertPlanarFloat32RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            rgba[j + 0] = (input[redInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 1] = (input[greenInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 2] = (input[blueInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    async function loadModel(modelName/*:String*/, executionProvider/*:String*/) {
        let modelPath;
        let modelSession;
        let freeDimensionOverrides;

        Utils.log(`Loading model ${modelName}...`);
        if (modelName == 'text-encoder') {
            //  Inputs:
            //    int32 input_ids[batch,sequence]
            //    batch: 2
            //    sequence: 77
            //  Outputs:
            //    float16 last_hidden_state[Addlast_hidden_state_dim_0,Addlast_hidden_state_dim_1,768]
            //    float16 pooler_output[Addlast_hidden_state_dim_0,768] We don't care about this ignorable output.
            //    Addlast_hidden_state_dim_0: 2
            //    Addlast_hidden_state_dim_1: 77
            // modelPath = 'models/Stable-Diffusion-v1.5-text-encoder-float16.onnx';
            modelPath = Utils.modelPath() + 'stable-diffusion/text-encoder.onnx'
            freeDimensionOverrides = {
                'batch': unetBatch,
                'sequence': textEmbeddingSequenceLength,
            }
        }
        else if (modelName == 'stable-diffusion-unet') {
            //  Typical shapes (some models may vary, like inpainting have 9 channels or single batch having 1 batch)...
            //
            //  Inputs:
            //    float16 sample[2, 4, 64, 64]
            //    int64 timestep[2]
            //    float16 encoder_hidden_states[2, 77, 768]
            //  Outputs:
            //    float16 out_sample[2, 4, 64, 64]
            modelPath = Utils.modelPath() + 'stable-diffusion/sd-unet-v1.5-model-b2c4h64w64s77-float16-compute-and-inputs.onnx';

            freeDimensionOverrides =
            {
                'batch': unetBatch,
                'channels': unetChannelCount,
                'height': latentHeight,
                'width': latentWidth,
                'sequence': textEmbeddingSequenceLength,
                'unet_sample_batch': unetBatch,
                'unet_sample_channels': unetChannelCount,
                'unet_sample_height': latentHeight,
                'unet_sample_width': latentWidth,
                'unet_time_batch': unetBatch,
                'unet_hidden_batch': unetBatch,
                'unet_hidden_sequence': textEmbeddingSequenceLength,
            };
        }
        else if (modelName == 'stable-diffusion-vae-decoder') {
            //  Inputs:
            //    float16 latent_sample[1, 4, 64, 64]
            //  Outputs:
            //    float16 sample[1, 3, 512, 512]
            modelPath = Utils.modelPath() + 'stable-diffusion/Stable-Diffusion-v1.5-vae-decoder-float16-fp32-instancenorm.onnx';
            freeDimensionOverrides = { 'batch': 1, 'channels': latentChannelCount, 'height': latentHeight, 'width': latentWidth, };
        }
        else {
            throw new Error(`Model ${modelName} is unknown`);
        }

        const options =
        {
            executionProviders: [{ name: executionProvider, deviceType: Utils.getQueryVariable('device', 'gpu'), powerPreference: 'default' }],
        };

        if (freeDimensionOverrides != undefined) {
            options.freeDimensionOverrides = freeDimensionOverrides;
        }

        options.logSeverityLevel = 3;

        Utils.log('Model path = ' + modelPath);
        Utils.log('Creating session...');
        let modelBuffer;
        modelBuffer = await Utils.getModelOPFS(modelName, modelPath, false);
        let createStartTime = performance.now();
        modelSession = await ort.InferenceSession.create(modelBuffer, options);
        if (modelName == 'text-encoder') {
            performanceData.sessioncreate.textencoder = (performance.now() - createStartTime).toFixed(2);
        } else if (modelName == 'stable-diffusion-unet') {
            performanceData.sessioncreate.unet = (performance.now() - createStartTime).toFixed(2);
        } else if (modelName == 'stable-diffusion-vae-decoder') {
            performanceData.sessioncreate.vaedecode = (performance.now() - createStartTime).toFixed(2);
        }
        Utils.log('...session created');
        return modelSession;
    }

    function displayEmptyCanvasPlaceholder() {
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');
        context.fillStyle = 'rgba(255, 255, 255, 0.5)';
        context.strokeStyle = 'rgba(255, 255, 255, 0.0)';
        context.lineWidth = 0;
        //context.fillRect(0, 0, pixelWidth, pixelHeight);
        context.textAlign = 'center';
        context.textBaseline = 'middle';
        context.font = '300px sans-serif';
        context.fillText('ðŸ–¼ï¸', canvas.width / 2, canvas.height / 2);
        context.strokeRect(0, 0, pixelWidth, pixelHeight);
    }

    function displayPlanarRGB(planarPixelData/*: Float32Array or Uint16Array as float16 or Uint8Array*/) {
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');

        // TODO: See if ORT's toImageData() is flexible enough to handle this instead.
        // It doesn't appear work correctly, just returning all white (shrug, maybe I'm passing the wrong values).
        // https://onnxruntime.ai/docs/api/js/interfaces/Tensor-1.html#toImageData
        // https://github.com/microsoft/onnxruntime/blob/5228332/js/common/lib/tensor-conversion.ts#L33
        // https://github.com/microsoft/onnxruntime/blob/main/js/common/lib/tensor-factory.ts#L147
        //
        // let imageData = planarPixelTensor.toImageData({format: 'RGB', tensorLayout: 'NCHW', norm:{bias: 1, mean: 128}});

        let conversionFunction = planarPixelData instanceof Float32Array
            ? convertPlanarFloat32RgbToUint8Rgba
            : planarPixelData instanceof Uint16Array
                ? convertPlanarFloat16RgbToUint8Rgba
                : convertPlanarUint8RgbToUint8Rgba;

        let rgbaPixels = conversionFunction(planarPixelData, pixelWidth, pixelHeight);

        let imageData = new ImageData(rgbaPixels, pixelWidth, pixelHeight);
        context.putImageData(imageData, 0, 0);
    }

    let textEncoderSession;
    let vaeDecodeModelSession;
    let unetModelSession;

    // Hard-coded values for 25 iterations (the standard).
    const defaultSigmas/*[25 + 1]*/ = [14.614647, 11.435942, 9.076809, 7.3019943, 5.9489183, 4.903778, 4.0860896, 3.4381795, 2.9183085, 2.495972, 2.1485956, 1.8593576, 1.6155834, 1.407623, 1.2280698, 1.0711612, 0.9323583, 0.80802417, 0.695151, 0.5911423, 0.49355352, 0.3997028, 0.30577788, 0.20348993, 0.02916753, 0.0];
    const defaultTimeSteps/*[25]*/ = [999.0, 957.375, 915.75, 874.125, 832.5, 790.875, 749.25, 707.625, 666.0, 624.375, 582.75, 541.125, 499.5, 457.875, 416.25, 374.625, 333.0, 291.375, 249.75, 208.125, 166.5, 124.875, 83.25, 41.625, 0.0];

    async function initializeOnnxRuntime() {
        // Global singletons -_-. Initialize ORT's global singleton.
        ort.env.wasm.numThreads = 1; // 4
        ort.env.wasm.simd = true;
        ort.env.wasm.proxy = false;
    }

    async function loadStableDiffusion(executionProvider, progress) {
        try {
            // Release sessions if load models again.
            if (textEncoderSession) {
                await unetModelSession.release();
                await textEncoderSession.release();
                await vaeDecodeModelSession.release();
            }

            error.removeAttribute("class");
            error.innerHTML = '';
            progress += 10;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Loading text-encoder model... " + progress.toFixed(2) + "%";
            const loadStartTime = performance.now();
            textEncoderSession = await loadModel('text-encoder', executionProvider);
            performanceData.loadtime.textencoder = (performance.now() - loadStartTime).toFixed(2);
            progress += 80;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Loading unet model... " + progress.toFixed(2) + "%";
            const unetLoadStartTime = performance.now();
            unetModelSession = await loadModel('stable-diffusion-unet', executionProvider);
            performanceData.loadtime.unet = (performance.now() - unetLoadStartTime).toFixed(2);
            progress += 5;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Loading vae-decoder model... " + progress.toFixed(2) + "%";
            const vaeDecodeLoadStartTime = performance.now();
            vaeDecodeModelSession = await loadModel('stable-diffusion-vae-decoder', executionProvider);
            performanceData.loadtime.vaedecode = (performance.now() - vaeDecodeLoadStartTime).toFixed(2);
            progress += 5;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Models loaded... " + progress.toFixed(2) + "%";
            const loadTime = performance.now() - loadStartTime;
            Utils.log(`Total load time: ${(loadTime / 1000).toFixed(2)}s`);
            performanceData.loadtime.total = loadTime.toFixed(2);
            startButton.removeAttribute('disabled');
        }
        catch (e) {
            console.log('Exception: ', e);
            error.setAttribute("class", "error");
            error.innerHTML = e.message;
            Utils.appendStatus('Exception: ' + e);
        }
    }

    function practRandSimpleFastCounter32(a, b, c, d)
    // https://pracrand.sourceforge.net/
    // Using this as a substitute for std::minstd_rand instead.
    // (std::linear_congruential_engine<std::uint_fast32_t, 48271, 0, 2147483647>).
    {
        return function () {
            a >>>= 0; b >>>= 0; c >>>= 0; d >>>= 0;
            var t = (a + b) | 0;
            a = b ^ b >>> 9;
            b = c + (c << 3) | 0;
            c = (c << 21 | c >>> 11);
            d = d + 1 | 0;
            t = t + d | 0;
            c = c + t | 0;
            return (t >>> 0) / 4294967296;
        };
    }

    function generateNoise(/*out*/ latentSpace/*: Uint16Array*/, seed/*: BigInt*/) {
        // Don't know nearly equivalent to .

        let randomGenerator = practRandSimpleFastCounter32(
            Number(seed >> 0n) & 0xFFFFFFFF,
            Number(seed >> 32n) & 0xFFFFFFFF,
            Number(seed >> 64n) & 0xFFFFFFFF,
            Number(seed >> 96n) & 0xFFFFFFFF
        );

        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            const u1 = randomGenerator();
            const u2 = randomGenerator();
            const radius = Math.sqrt(-2.0 * Math.log(u1));
            const theta = 2.0 * Math.PI * u2;
            const standardNormalRand = radius * Math.cos(theta);
            const newValue = standardNormalRand;
            latentSpace[i] = Utils.encodeFloat16(newValue);
        }
    }

    function prescaleLatentSpace(/*inout*/ latentSpace/*: Uint16Array*/, initialSigma/*: float*/) {
        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) * initialSigma);
        }
    }

    function scaleLatentSpaceForPrediction(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: int*/) {
        console.assert(iterationIndex < defaultSigmas.length);

        // sample = sample / ((sigma**2 + 1) ** 0.5)
        let sigma = defaultSigmas[iterationIndex];
        let inverseScale = 1 / Math.sqrt(sigma * sigma + 1);

        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) * inverseScale);
        }
    }

    // Adjusts the latent space in-place by the predicted noise, weighted for the current iteration.
    // This version takes two batches, with the positive prediction in batch 0, negative in batch 1.
    function denoiseLatentSpace(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: Number*/, predictedNoise/*: Uint16Array*/) {
        console.assert(latentSpace.length === predictedNoise.length);

        const elementCount = latentSpace.length;          // Given [2, 4, 64, 64], count of all elements.
        const singleBatchElementCount = elementCount / 2; // Given [2, 4, 64, 64], we want only the first batch.

        // Prompt strength scale.
        const defaultPromptStrengthScale = 7.5;
        const positiveWeight = defaultPromptStrengthScale;
        const negativeWeight = 1 - positiveWeight;

        // Add predicted noise (scaled by current iteration weight) to latents.
        const sigma = defaultSigmas[iterationIndex];
        const sigmaNext = defaultSigmas[iterationIndex + 1];
        const dt = sigmaNext - sigma;

        for (let i = 0; i < singleBatchElementCount; ++i) {
            // Fold 2 batches into one, weighted by positive and negative weights.
            const weightedPredictedNoise = Utils.decodeFloat16(predictedNoise[i]) * positiveWeight + Utils.decodeFloat16(predictedNoise[i + singleBatchElementCount]) * negativeWeight;

            // The full formula:
            //
            //  // 1. Compute predicted original sample from sigma-scaled predicted noise.
            //  float sample = latentSpace[i];
            //  float predictedOriginalSample = sample - sigma * predictedNoiseData[i];
            // 
            //  // 2. Convert to an ODE derivative
            //  float derivative = (sample - predictedOriginalSample) / sigma;
            //  float previousSample = sample + derivative * dt;
            //  latentSpace[i] = previousSample;
            //
            // Simplifies to:
            //
            //  updatedSample = sample + ((sample - (sample - sigma * predictedNoiseData[i])) / sigma  * dt);
            //  updatedSample = sample + ((sample - sample + sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + ((sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + (predictedNoiseData[i] * dt);

            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) + weightedPredictedNoise * dt);
        }
    }

    // Adjusts the latent space in-place by the predicted noise, weighted for the current iteration.
    // This version takes two separate predicted noise arrays.
    function denoiseLatentSpaceSplitPredictions(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: Number*/, positivePredictedNoise/*: Uint16Array*/, negativePredictedNoise/*: Uint16Array*/) {
        console.assert(latentSpace.length === positivePredictedNoise.length);
        console.assert(latentSpace.length === negativePredictedNoise.length);

        const elementCount = latentSpace.length;          // Given [2, 4, 64, 64], count of all elements.

        // Prompt strength scale.
        const defaultPromptStrengthScale = 7.5;
        const positiveWeight = defaultPromptStrengthScale;
        const negativeWeight = 1 - positiveWeight;

        // Add predicted noise (scaled by current iteration weight) to latents.
        const sigma = defaultSigmas[iterationIndex];
        const sigmaNext = defaultSigmas[iterationIndex + 1];
        const dt = sigmaNext - sigma;

        for (let i = 0; i < elementCount; ++i) {
            // Fold 2 batches into one, weighted by positive and negative weights.
            const weightedPredictedNoise = Utils.decodeFloat16(positivePredictedNoise[i]) * positiveWeight + Utils.decodeFloat16(negativePredictedNoise[i]) * negativeWeight;

            // The full formula:
            //
            //  // 1. Compute predicted original sample from sigma-scaled predicted noise.
            //  float sample = latentSpace[i];
            //  float predictedOriginalSample = sample - sigma * predictedNoiseData[i];
            // 
            //  // 2. Convert to an ODE derivative
            //  float derivative = (sample - predictedOriginalSample) / sigma;
            //  float previousSample = sample + derivative * dt;
            //  latentSpace[i] = previousSample;
            //
            // Simplifies to:
            //
            //  updatedSample = sample + ((sample - (sample - sigma * predictedNoiseData[i])) / sigma  * dt);
            //  updatedSample = sample + ((sample - sample + sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + ((sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + (predictedNoiseData[i] * dt);

            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) + weightedPredictedNoise * dt);
        }
    }

    function applyVaeScalingFactor(latentSpace/*: Uint16Array as float16*/) {
        const /*float*/ defaultVaeScalingFactor = 0.18215; // Magic constants for default VAE :D (used in Huggingface pipeline).
        const /*float*/ inverseScalingFactor = 1.0 / defaultVaeScalingFactor;
        latentSpace.forEach((e, i, a) => a[i] = Utils.encodeFloat16(Utils.decodeFloat16(e) * inverseScalingFactor));
    }

    async function executeStableDiffusion()/*: ort.Tensor*/
    // Implicit inputs:
    // - unetModelSession
    // - unetInputs
    // - vaeDecodeInputs
    {
        Utils.log('Beginning Text encode...');
        let token_ids = await getTextTokens();
        const startTextEncoder = performance.now();
        const textEncoderInputs = {
            'input_ids': Utils.generateTensorFromValues('int32', [unetBatch, textEmbeddingSequenceLength], token_ids),
        };
        const textEncoderOutputs = await textEncoderSession.run(textEncoderInputs);
        performanceData.sessionrun.textencoder = (performance.now() - startTextEncoder).toFixed(2);
        Utils.log(`Text encode execution time: ${(performance.now() - startTextEncoder).toFixed(2)}ms`);

        inferenceProgress += 1;
        progressBarInnerInference.style.width = inferenceProgress + "%";
        progressBarLabelInference.textContent = "Text encoded...... " + inferenceProgress.toFixed(2) + "%";

        Utils.log('Beginning Unet loop execution for 25 iterations...');

        let latentSpace = new Uint16Array(latentWidth * latentHeight * unetChannelCount);
        generateNoise(/*inout*/ latentSpace, seed);
        // Duplicate the input data, once for each batch (only supports unetBatch == 2).
        latentSpace = new Uint16Array([...latentSpace, ...latentSpace]);

        const latentsTensor = Utils.generateTensorFromBytes(
            'float16',
            [unetBatch, unetChannelCount, latentHeight, latentWidth],
            latentSpace);

        const halfLatentElementCount = latentsTensor.size / 2; // Given [2, 4, 64, 64], we want only the first batch.
        let latents = await latentsTensor.getData();
        let halfLatents = latents.subarray(0, halfLatentElementCount); // First batch only.
        prescaleLatentSpace(/*inout*/ halfLatents, defaultSigmas[0]);

        const unetInputs = {
            'encoder_hidden_states': Utils.generateTensorFromBytes('float16',
                [unetBatch, textEmbeddingSequenceLength, textEmbeddingSequenceWidth],
                textEncoderOutputs['last_hidden_state'].data),
        };

        const startUnet = performance.now();
        // Repeat unet detection and denosing until convergence (typically 25 iterations).
        for (var i = 0; i < unetIterationCount; ++i) {
            // Update time step.
            let startUnetIteration = performance.now();
            const timeStepValue = BigInt(Math.round(defaultTimeSteps[i])); // Round, because this ridiculous language throws an exception otherwise.
            unetInputs['timestep'] = Utils.generateTensorFillValue('int64', [unetBatch], timeStepValue);

            // Prescale the latent values.
            // Copy first batch to second batch, duplicating latents for positive and negative prompts.
            let nextLatents = latents.slice(0);
            let halfNextLatents = nextLatents.subarray(0, halfLatentElementCount);
            scaleLatentSpaceForPrediction(/*inout*/ halfNextLatents, i);
            nextLatents.copyWithin(halfLatentElementCount, 0, halfLatentElementCount); // Copy lower half to upper half.

            unetInputs['sample'] = Utils.generateTensorFromBytes('float16', [unetBatch, unetChannelCount, latentHeight, latentWidth], nextLatents);
            const unetOutputs = await unetModelSession.run(unetInputs);
            let time = (performance.now() - startUnetIteration).toFixed(2);
            performanceData.sessionrun.unet.push(time);
            let predictedNoise = new Uint16Array(unetOutputs['out_sample'].cpuData.buffer);

            denoiseLatentSpace(/*inout*/ latents, i, predictedNoise);

            inferenceProgress += 3.8;
            progressBarInnerInference.style.width = inferenceProgress + "%";
            progressBarLabelInference.textContent = `UNet iteration ${i + 1} completed...... ${inferenceProgress.toFixed(2)}%`;
        }

        performanceData.sessionrun.unettotal = (performance.now() - startUnet).toFixed(2);
        Utils.log(`Unet loop execution time: ${(performance.now() - startUnet).toFixed(2)}ms`);

        Utils.log('Beginning VAE decode...');
        // Decode from latent space.
        applyVaeScalingFactor(/*inout*/ halfLatents);
        let dimensions = latentsTensor.dims.slice(0);
        dimensions[0] = 1; // Set batch size to 1, ignore the 2nd batch for the negative prediction.

        const startVaeDecoder = performance.now();
        const vaeDecodeInputs = {
            'latent_sample': Utils.generateTensorFromBytes('float16', dimensions, halfLatents.slice(0)),
        };
        const decodedOutputs = await vaeDecodeModelSession.run(vaeDecodeInputs);
        Utils.log(`VAE decode execution time: ${(performance.now() - startVaeDecoder).toFixed(2)}ms`);
        performanceData.sessionrun.vaedecode = (performance.now() - startVaeDecoder).toFixed(2);

        inferenceProgress += 4;
        progressBarInnerInference.style.width = inferenceProgress + "%";
        progressBarLabelInference.textContent = "VAE decoded ......" + inferenceProgress.toFixed(2) + "%";

        return decodedOutputs['sample'];
    }

    async function executeStableDiffusionAndDisplayOutput() {
        try {
            displayEmptyCanvasPlaceholder();

            const executionStartTime = performance.now();
            let rgbPlanarPixels = await executeStableDiffusion();
            const executionTime = performance.now() - executionStartTime;
            performanceData.sessionrun.total = executionTime.toFixed(2);
            Utils.log(`Total execution time: ${(executionTime / 1000).toFixed(2)}s`);
            console.log(performanceData);
            displayPlanarRGB(await rgbPlanarPixels.getData());
        }
        catch (e) {
            console.log('Exception: ', e);
            Utils.appendStatus('Exception: ' + e);
        }
    }

    async function generateNextImage() {
        data.removeAttribute('class');
        data.setAttribute('class', 'hide');
        await executeStableDiffusionAndDisplayOutput();
        // seed++;
        console.log(seed);
        startButton.disabled = false;

        if (performanceData.sessionrun.total) {
            textencoderload.innerHTML = performanceData.loadtime.textencoder
            textencodercreate.innerHTML = performanceData.sessioncreate.textencoder;
            textencoderrun.innerHTML = performanceData.sessionrun.textencoder;

            unetload.innerHTML = performanceData.loadtime.unet;
            unetcreate.innerHTML = performanceData.sessioncreate.unet;
            unetrun.innerHTML = performanceData.sessionrun.unet.toString().replaceAll(',', ' ') + ' / ' + performanceData.sessionrun.unettotal;

            vaedecodeload.innerHTML = performanceData.loadtime.vaedecode;
            vaedecodecreate.innerHTML = performanceData.sessioncreate.vaedecode;
            vaedecoderun.innerHTML = performanceData.sessionrun.vaedecode;

            totalload.innerHTML = performanceData.loadtime.total;
            // totalcreate.innerHTML = performanceData.sessioncreate.total;
            totalrun.innerHTML = performanceData.sessionrun.total;
        }

        data.setAttribute('class', 'show');
    }

    const executionProvider = Utils.getQueryVariable('provider', 'webnn');
    Utils.log('Execution Provider: ' + executionProvider);

    displayEmptyCanvasPlaceholder();

    initializeOnnxRuntime();

    const checkWebNN = async () => {
        let status = document.querySelector('#webnnstatus');
        let info = document.querySelector('#info');
        let webnnStatus = await Utils.webNnStatus();

        if (webnnStatus.webnn) {
            status.setAttribute('class', 'green');
            info.innerHTML = 'WebNN Supported';
        } else {
            if (webnnStatus.error) {
                status.setAttribute('class', 'red');
                info.innerHTML = 'WebNN not supported: ' + webnnStatus.error;
            } else {
                status.setAttribute('class', 'red');
                info.innerHTML = 'WebNN not supported';
            }
        }

        if (Utils.getQueryValue('provider') && Utils.getQueryValue('provider').toLowerCase().indexOf('webgpu') > -1) {
            status.innerHTML = '';
        }
    };

    const ui = async () => {
        if (Utils.getQueryValue('provider') && Utils.getQueryValue('provider').toLowerCase().indexOf('webgpu') > -1) {
            title.innerHTML = 'WebGPU';
        }
        await checkWebNN();
    };

    document.addEventListener('DOMContentLoaded', ui, false);

    const updateSeed = () => {
        userSeed.value = Utils.randomNumber();
        seed = BigInt(userSeed.value);
    }

    changeSeed.addEventListener('click', updateSeed, false);
</script>

</html>