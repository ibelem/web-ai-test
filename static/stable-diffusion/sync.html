<html>
<!--
    Usage:
        http://localhost:8000/sd-demo.html // defaults to WebNN GPU
        http://localhost:8000/sd-demo.html?provider=webnn&device=gpu
        http://localhost:8000/sd-demo.html?provider=webnn&device=cpu
        http://localhost:8000/sd-demo.html?provider=wasm

        provider = wasm | webnn | webgpu | webgl
        device = cpu | gpu | npu // applicable to WebNN
-->

<head>
    <meta charset='utf-8'>
    <title>WebNN Stable Diffusion</title>
    <link rel="stylesheet" href="main.css">
</head>

<body>
    <div>
        <h1>WebNN Stable Diffusion</h1>
    </div>
    <div class="container">

        <div class="left">
            <div class="input-group">
                <label for="prompt">Prompt:</label>
                <input type="text" id="positive_prompt" value="giant castle, mountains, sunrise, volumetric lighting">
            </div>
            <div class="input-group">
                <label for="negative-prompt">Negative Prompt:</label>
                <input type="text" id="negative_prompt">
            </div>
            <div class="button-group">
                <button id="load_models" class="button">Load Models</button>
                <button id="generate_next_image" class="button" disabled>Generate Next Image</button>
            </div>
            <div class="progress-bar">
                <div id="progress-bar-inner" class="progress-bar-inner"></div>
            </div>
            <div class="progress-bar-label" id="progress-bar-label">0%</div>
            <div class="log-output" id='status'>
            </div>
        </div>
        <div class="right">
            <canvas class='canvas' id='canvas' width='512' height='512'>
                Canvas is not supported. You'll need to try a newer browser version or another browser.
            </canvas>
        </div>
    </div>
</body>

<script src='dist/ort.min.js'></script>
<script type='module'>
    import { AutoTokenizer } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.13.4';
    import * as Utils from './utils.js';

    // Configuration...
    const pixelWidth = 512;
    const pixelHeight = 512;
    const latentWidth = (pixelWidth / 8);
    const latentHeight = (pixelHeight / 8);
    const latentChannelCount = 4;
    const unetBatch = 2;
    const unetChannelCount = 4;
    const textEmbeddingSequenceLength = 77;
    const textEmbeddingSequenceWidth = 768;
    const unetIterationCount = 25; // Hard-coded number of samples, since the denoising weight ramp is constant.
    let seed = BigInt(123465n);
    const enablePrintDebugging = false;
    const enableNanContaminationDebugging = false;

    function getQueryVariable(name, defaults) {
        const query = window.location.search.substring(1);
        let vars = query.split('&');
        for (var i = 0; i < vars.length; i++) {
            let pair = vars[i].split('=');
            if (pair[0] == name) {
                return pair[1];
            }
        }
        return defaults;
    }

    Utils.log('Loading ONNX Runtime...');
    const progressBarInner = document.getElementById("progress-bar-inner");
    const progressBarLabel = document.getElementById("progress-bar-label");
    const startButton = document.getElementById('generate_next_image');
    const loadButton = document.getElementById('load_models');
    const logOutput = document.getElementById("status");
    let progress = 0;

    logOutput.addEventListener("DOMSubtreeModified", function () {
        logOutput.scrollTop = logOutput.scrollHeight;
    });

    loadButton.onclick = async () => {
        await loadStableDiffusion(executionProvider, progress);
        if (progress >= 100) {
            progress = 0;
        }
    }
    startButton.onclick = async () => {
        startButton.disabled = true;
        await generateNextImage();
    }

    async function getTextTokens() {
        let path = Utils.modelPath() + 'stable-diffusion/tokenizer/';
        if (location.href.toLowerCase().indexOf('github.io') > -1 
            || location.href.toLowerCase().indexOf('huggingface.co') > -1
            || location.href.toLowerCase().indexOf('vercel.app') > -1
            || location.href.toLowerCase().indexOf('netlify.app') > -1
            || location.href.toLowerCase().indexOf('webai.run') > -1) {
            path = `stable-diffusion/tokenizer`;
        }
        const tokenizers = await AutoTokenizer.from_pretrained(path);
        const positiveText = document.getElementById('positive_prompt').value;
        const negativeText = document.getElementById('negative_prompt').value;

        // A string like 'a cute magical flying ghost dog, fantasy art, golden color, high quality, highly detailed, elegant, sharp focus, concept art, character concepts, digital painting, mystery, adventure'
        // becomes a 1D tensor of {49406, 320, 2242, 7823, 4610, 7108, 1929, 267, 5267, 794, 267, 3878, 3140, 267, 1400, 3027, ...}
        // padded with blanks (id 49407) up to the maximum sequence length of the text encoder (typically 77).
        // So the text encoder can't really handle more than 75 words (+1 start, +1 stop token),
        // not without some extra tricks anyway like calling it multiple times and combining the embeddings.
        const { input_ids } = await tokenizers(positiveText);
        let positive_token_ids = [49406]; // Inits with start token
        let negative_token_ids = [49406];
        let input_ids_array = Array.from(input_ids.data, number => Number(number)).flat();
        positive_token_ids = positive_token_ids.concat(input_ids_array);
        if (input_ids_array.length > (textEmbeddingSequenceLength - 2)) { // Max inputs ids should be 75
            positive_token_ids = positive_token_ids.slice(0, (textEmbeddingSequenceLength - 1));
            positive_token_ids.push(49407);
        } else {
            const fillerArray = new Array(textEmbeddingSequenceLength - positive_token_ids.length).fill(49407);
            positive_token_ids = positive_token_ids.concat(fillerArray);
        }

        const negative_ids = await tokenizers(negativeText);
        let negative_ids_array = Array.from(negative_ids.input_ids.data, number => Number(number)).flat();
        negative_token_ids = negative_token_ids.concat(negative_ids_array);
        if (negative_ids_array.length > (textEmbeddingSequenceLength - 2)) {
            negative_token_ids = negative_token_ids.slice(0, (textEmbeddingSequenceLength - 1));
            negative_token_ids.push(49407);
        } else {
            const fillerArray = new Array(textEmbeddingSequenceLength - negative_token_ids.length).fill(49407);
            negative_token_ids = negative_token_ids.concat(fillerArray);
        }

        const token_ids = positive_token_ids.concat(negative_token_ids);
        return token_ids;
    };

    Utils.log('...loaded');

    function convertPlanarFloat16RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            rgba[j + 0] = (Utils.decodeFloat16(input[redInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 1] = (Utils.decodeFloat16(input[greenInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 2] = (Utils.decodeFloat16(input[blueInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    function convertPlanarUint8RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            let inputValue = input[redInputOffset + i];
            rgba[j + 0] = inputValue;
            rgba[j + 1] = inputValue;
            rgba[j + 2] = inputValue;
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    function convertPlanarFloat32RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            rgba[j + 0] = (input[redInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 1] = (input[greenInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 2] = (input[blueInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    async function loadModel(modelName/*:String*/, executionProvider/*:String*/) {
        let modelPath;
        let modelSession;
        let freeDimensionOverrides;
        let inputs = {};

        Utils.log(`Loading model ${modelName}...`);
        if (modelName == 'text-encoder') {
            //  Inputs:
            //    int32 input_ids[batch,sequence]
            //    batch: 2
            //    sequence: 77
            //  Outputs:
            //    float16 last_hidden_state[Addlast_hidden_state_dim_0,Addlast_hidden_state_dim_1,768]
            //    float16 pooler_output[Addlast_hidden_state_dim_0,768] We don't care about this ignorable output.
            //    Addlast_hidden_state_dim_0: 2
            //    Addlast_hidden_state_dim_1: 77
            // modelPath = 'models/Stable-Diffusion-v1.5-text-encoder-float16.onnx';
            modelPath = Utils.modelPath() + 'stable-diffusion/text-encoder.onnx'
            freeDimensionOverrides = {
                'batch': unetBatch,
                'sequence': textEmbeddingSequenceLength,
            }
        }
        else if (modelName == 'stable-diffusion-unet') {
            //  Typical shapes (some models may vary, like inpainting have 9 channels or single batch having 1 batch)...
            //
            //  Inputs:
            //    float16 sample[2, 4, 64, 64]
            //    int64 timestep[2]
            //    float16 encoder_hidden_states[2, 77, 768]
            //  Outputs:
            //    float16 out_sample[2, 4, 64, 64]
            modelPath = Utils.modelPath() + 'stable-diffusion/sd-unet-v1.5-model-b2c4h64w64s77-float16-compute-and-inputs.onnx';

            let latentSpace = new Uint16Array(latentWidth * latentHeight * unetChannelCount);
            generateNoise(/*inout*/ latentSpace, seed);

            // Duplicate the input data, once for each batch (only supports unetBatch == 2).
            let oldLatentSpace = latentSpace;
            latentSpace = new Uint16Array(oldLatentSpace.length * 2);
            latentSpace.set(oldLatentSpace); // Write lower half.
            latentSpace.set(oldLatentSpace, oldLatentSpace.length); // Repeat in upper half.

            inputs['sample'] = Utils.generateTensorFromBytes('float16', [unetBatch, unetChannelCount, latentHeight, latentWidth], latentSpace);
            inputs['timestep'] = Utils.generateTensorFillValue('int64', [unetBatch], 0n);

            freeDimensionOverrides =
            {
                'batch': unetBatch,
                'channels': unetChannelCount,
                'height': latentHeight,
                'width': latentWidth,
                'sequence': textEmbeddingSequenceLength,
                'unet_sample_batch': unetBatch,
                'unet_sample_channels': unetChannelCount,
                'unet_sample_height': latentHeight,
                'unet_sample_width': latentWidth,
                'unet_time_batch': unetBatch,
                'unet_hidden_batch': unetBatch,
                'unet_hidden_sequence': textEmbeddingSequenceLength,
            };
        }
        else if (modelName == 'stable-diffusion-vae-decoder') {
            //  Inputs:
            //    float16 latent_sample[1, 4, 64, 64]
            //  Outputs:
            //    float16 sample[1, 3, 512, 512]
            modelPath = Utils.modelPath() + 'stable-diffusion/Stable-Diffusion-v1.5-vae-decoder-float16-fp32-instancenorm.onnx';
            // modelPath = 'models/vae-decoder.onnx';
            inputs['latent_sample'] = Utils.generateTensorFillValue('float16', [1, latentChannelCount, latentHeight, latentWidth], 1);
            freeDimensionOverrides = { 'batch': 1, 'channels': latentChannelCount, 'height': latentHeight, 'width': latentWidth, };
        }
        else {
            throw new Error(`Model ${modelName} is unknown`);
        }

        const options =
        {
            executionProviders: [{ name: executionProvider, deviceType: getQueryVariable('device', 'gpu'), powerPreference: 'default' }],
        };

        if (freeDimensionOverrides != undefined) {
            options.freeDimensionOverrides = freeDimensionOverrides;
        }

        options.logSeverityLevel = 3;

        Utils.log('Model path = ' + modelPath);
        Utils.log('Creating session...');
        let modelBuffer;
        modelBuffer = await Utils.getModelOPFS(modelName, modelPath, false);
        modelSession = await ort.InferenceSession.create(modelBuffer, options);
        Utils.log('...session created');

        return [modelSession, inputs]
    }

    function displayEmptyCanvasPlaceholder() {
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');
        context.fillStyle = 'rgba(255, 255, 255, 0.5)';
        context.strokeStyle = 'rgba(255, 255, 255, 0.0)';
        context.lineWidth = 0;
        //context.fillRect(0, 0, pixelWidth, pixelHeight);
        context.textAlign = 'center';
        context.textBaseline = 'middle';
        context.font = '300px sans-serif';
        context.fillText('ðŸ–¼ï¸', canvas.width / 2, canvas.height / 2);
        context.strokeRect(0, 0, pixelWidth, pixelHeight);
    }

    function displayPlanarRGB(planarPixelData/*: Float32Array or Uint16Array as float16 or Uint8Array*/) {
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');

        // TODO: See if ORT's toImageData() is flexible enough to handle this instead.
        // It doesn't appear work correctly, just returning all white (shrug, maybe I'm passing the wrong values).
        // https://onnxruntime.ai/docs/api/js/interfaces/Tensor-1.html#toImageData
        // https://github.com/microsoft/onnxruntime/blob/5228332/js/common/lib/tensor-conversion.ts#L33
        // https://github.com/microsoft/onnxruntime/blob/main/js/common/lib/tensor-factory.ts#L147
        //
        // let imageData = planarPixelTensor.toImageData({format: 'RGB', tensorLayout: 'NCHW', norm:{bias: 1, mean: 128}});

        let conversionFunction = planarPixelData instanceof Float32Array
            ? convertPlanarFloat32RgbToUint8Rgba
            : planarPixelData instanceof Uint16Array
                ? convertPlanarFloat16RgbToUint8Rgba
                : convertPlanarUint8RgbToUint8Rgba;

        let rgbaPixels = conversionFunction(planarPixelData, pixelWidth, pixelHeight);

        let imageData = new ImageData(rgbaPixels, pixelWidth, pixelHeight);
        context.putImageData(imageData, 0, 0);
    }

    let textEncoderSession;
    let vaeDecodeModelSession;
    let unetModelSession;
    let textEncoderInputs;
    let unetInputs;
    let vaeDecodeInputs;
    let textEncoderOutputs;

    // Hard-coded values for 25 iterations (the standard).
    const defaultSigmas/*[25 + 1]*/ = [14.614647, 11.435942, 9.076809, 7.3019943, 5.9489183, 4.903778, 4.0860896, 3.4381795, 2.9183085, 2.495972, 2.1485956, 1.8593576, 1.6155834, 1.407623, 1.2280698, 1.0711612, 0.9323583, 0.80802417, 0.695151, 0.5911423, 0.49355352, 0.3997028, 0.30577788, 0.20348993, 0.02916753, 0.0];
    const defaultTimeSteps/*[25]*/ = [999.0, 957.375, 915.75, 874.125, 832.5, 790.875, 749.25, 707.625, 666.0, 624.375, 582.75, 541.125, 499.5, 457.875, 416.25, 374.625, 333.0, 291.375, 249.75, 208.125, 166.5, 124.875, 83.25, 41.625, 0.0];

    async function initializeOnnxRuntime() {
        // Global singletons -_-. Initialize ORT's global singleton.
        ort.env.wasm.numThreads = 1; // 4
        ort.env.wasm.simd = true;
        ort.env.wasm.proxy = true;
        // ort.env.logLevel = 'verbose'; //'error';
        // ort.env.debug = true;
    }

    async function loadStableDiffusion(executionProvider, progress) {
        try {
            progress += 10;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Loading text-encoder model..." + progress + "%";
            const loadStartTime = performance.now();
            [textEncoderSession, textEncoderInputs] = await loadModel('text-encoder', executionProvider);
            progress += 30;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Loading unet model..." + progress + "%";
            [unetModelSession, unetInputs] = await loadModel('stable-diffusion-unet', executionProvider);
            progress += 30;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Loading vae-decoder model..." + progress + "%";
            [vaeDecodeModelSession, vaeDecodeInputs] = await loadModel('stable-diffusion-vae-decoder', executionProvider);
            progress += 30;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Models loaded..." + progress + "%";
            const loadTime = performance.now() - loadStartTime;
            Utils.log(`Total load time: ${(loadTime / 1000).toFixed(2)}s`);

            startButton.removeAttribute('disabled');
        }
        catch (e) {
            console.log('Exception: ', e);
            Utils.appendStatus('Exception: ' + e);
        }
    }

    function practRandSimpleFastCounter32(a, b, c, d)
    // https://pracrand.sourceforge.net/
    // Using this as a substitute for std::minstd_rand instead.
    // (std::linear_congruential_engine<std::uint_fast32_t, 48271, 0, 2147483647>).
    {
        return function () {
            a >>>= 0; b >>>= 0; c >>>= 0; d >>>= 0;
            var t = (a + b) | 0;
            a = b ^ b >>> 9;
            b = c + (c << 3) | 0;
            c = (c << 21 | c >>> 11);
            d = d + 1 | 0;
            t = t + d | 0;
            c = c + t | 0;
            return (t >>> 0) / 4294967296;
        };
    }

    function generateNoise(/*out*/ latentSpace/*: Uint16Array*/, seed/*: BigInt*/) {
        // Don't know nearly equivalent to .

        let randomGenerator = practRandSimpleFastCounter32(
            Number(seed >> 0n) & 0xFFFFFFFF,
            Number(seed >> 32n) & 0xFFFFFFFF,
            Number(seed >> 64n) & 0xFFFFFFFF,
            Number(seed >> 96n) & 0xFFFFFFFF
        );

        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            const u1 = randomGenerator();
            const u2 = randomGenerator();
            const radius = Math.sqrt(-2.0 * Math.log(u1));
            const theta = 2.0 * Math.PI * u2;
            const standardNormalRand = radius * Math.cos(theta);
            const newValue = standardNormalRand;
            latentSpace[i] = Utils.encodeFloat16(newValue);
        }
    }

    function prescaleLatentSpace(/*inout*/ latentSpace/*: Uint16Array*/, initialSigma/*: float*/) {
        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) * initialSigma);
        }
    }

    function scaleLatentSpaceForPrediction(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: int*/) {
        console.assert(iterationIndex < defaultSigmas.length);

        // sample = sample / ((sigma**2 + 1) ** 0.5)
        let sigma = defaultSigmas[iterationIndex];
        let inverseScale = 1 / Math.sqrt(sigma * sigma + 1);

        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) * inverseScale);
        }
    }

    // Adjusts the latent space in-place by the predicted noise, weighted for the current iteration.
    // This version takes two batches, with the positive prediction in batch 0, negative in batch 1.
    function denoiseLatentSpace(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: Number*/, predictedNoise/*: Uint16Array*/) {
        console.assert(latentSpace.length === predictedNoise.length);

        const elementCount = latentSpace.length;          // Given [2, 4, 64, 64], count of all elements.
        const singleBatchElementCount = elementCount / 2; // Given [2, 4, 64, 64], we want only the first batch.

        // Prompt strength scale.
        const defaultPromptStrengthScale = 7.5;
        const positiveWeight = defaultPromptStrengthScale;
        const negativeWeight = 1 - positiveWeight;

        // Add predicted noise (scaled by current iteration weight) to latents.
        const sigma = defaultSigmas[iterationIndex];
        const sigmaNext = defaultSigmas[iterationIndex + 1];
        const dt = sigmaNext - sigma;

        if (enablePrintDebugging) {
            console.log(`latentSpace.length == ${latentSpace.length}`);
            console.log(`latentSpace[0] == ${latentSpace[0]} before`);
        }

        for (let i = 0; i < singleBatchElementCount; ++i) {
            // Fold 2 batches into one, weighted by positive and negative weights.
            const weightedPredictedNoise = Utils.decodeFloat16(predictedNoise[i]) * positiveWeight + Utils.decodeFloat16(predictedNoise[i + singleBatchElementCount]) * negativeWeight;
            if (enablePrintDebugging && i == 0) {
                console.log(`predictedNoise[i==${i}] == ${predictedNoise[i]} // positive prediction`);
                console.log(`predictedNoise[i + singleBatchElementCount == ${i + singleBatchElementCount}] == ${predictedNoise[i + singleBatchElementCount]} // negative prediction`);
                console.log(`weightedPredictedNoise[i==${i}] == ${weightedPredictedNoise}`);
                console.log();
            }

            // The full formula:
            //
            //  // 1. Compute predicted original sample from sigma-scaled predicted noise.
            //  float sample = latentSpace[i];
            //  float predictedOriginalSample = sample - sigma * predictedNoiseData[i];
            // 
            //  // 2. Convert to an ODE derivative
            //  float derivative = (sample - predictedOriginalSample) / sigma;
            //  float previousSample = sample + derivative * dt;
            //  latentSpace[i] = previousSample;
            //
            // Simplifies to:
            //
            //  updatedSample = sample + ((sample - (sample - sigma * predictedNoiseData[i])) / sigma  * dt);
            //  updatedSample = sample + ((sample - sample + sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + ((sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + (predictedNoiseData[i] * dt);

            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) + weightedPredictedNoise * dt);
            if (enablePrintDebugging && i == 0) {
                console.log(`latentSpace[i==${i}] == ${latentSpace[i]} after`);
            }
        }
    }

    // Adjusts the latent space in-place by the predicted noise, weighted for the current iteration.
    // This version takes two separate predicted noise arrays.
    function denoiseLatentSpaceSplitPredictions(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: Number*/, positivePredictedNoise/*: Uint16Array*/, negativePredictedNoise/*: Uint16Array*/) {
        console.assert(latentSpace.length === positivePredictedNoise.length);
        console.assert(latentSpace.length === negativePredictedNoise.length);

        const elementCount = latentSpace.length;          // Given [2, 4, 64, 64], count of all elements.

        // Prompt strength scale.
        const defaultPromptStrengthScale = 7.5;
        const positiveWeight = defaultPromptStrengthScale;
        const negativeWeight = 1 - positiveWeight;

        // Add predicted noise (scaled by current iteration weight) to latents.
        const sigma = defaultSigmas[iterationIndex];
        const sigmaNext = defaultSigmas[iterationIndex + 1];
        const dt = sigmaNext - sigma;

        if (enablePrintDebugging) {
            console.log(`latentSpace.length == ${latentSpace.length}`);
            console.log(`latentSpace[0] == ${latentSpace[0]} before`);
        }

        for (let i = 0; i < elementCount; ++i) {
            // Fold 2 batches into one, weighted by positive and negative weights.
            const weightedPredictedNoise = Utils.decodeFloat16(positivePredictedNoise[i]) * positiveWeight + Utils.decodeFloat16(negativePredictedNoise[i]) * negativeWeight;
            if (enablePrintDebugging && i == 0) {
                console.log(`positivePredictedNoise[i==${i}] == ${positivePredictedNoise[i]}`);
                console.log(`negativePredictedNoise[i==${i}] == ${negativePredictedNoise[i]}`);
                console.log(`weightedPredictedNoise[i==${i}] == ${weightedPredictedNoise}`);
            }

            // The full formula:
            //
            //  // 1. Compute predicted original sample from sigma-scaled predicted noise.
            //  float sample = latentSpace[i];
            //  float predictedOriginalSample = sample - sigma * predictedNoiseData[i];
            // 
            //  // 2. Convert to an ODE derivative
            //  float derivative = (sample - predictedOriginalSample) / sigma;
            //  float previousSample = sample + derivative * dt;
            //  latentSpace[i] = previousSample;
            //
            // Simplifies to:
            //
            //  updatedSample = sample + ((sample - (sample - sigma * predictedNoiseData[i])) / sigma  * dt);
            //  updatedSample = sample + ((sample - sample + sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + ((sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + (predictedNoiseData[i] * dt);

            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) + weightedPredictedNoise * dt);
            if (enablePrintDebugging && i == 0) {
                console.log(`latentSpace[i==${i}] == ${latentSpace[i]} after`);
            }
        }
    }

    function applyVaeScalingFactor(latentSpace/*: Uint16Array as float16*/) {
        const /*float*/ defaultVaeScalingFactor = 0.18215; // Magic constants for default VAE :D (used in Huggingface pipeline).
        const /*float*/ inverseScalingFactor = 1.0 / defaultVaeScalingFactor;
        latentSpace.forEach((e, i, a) => a[i] = Utils.encodeFloat16(Utils.decodeFloat16(e) * inverseScalingFactor));
    }

    async function executeStableDiffusion()/*: ort.Tensor*/
    // Implicit inputs:
    // - unetModelSession
    // - unetInputs
    // - vaeDecodeInputs
    {
        Utils.log('Beginning Text encode...');
        let token_ids = await getTextTokens();
        textEncoderInputs = { 'input_ids': Utils.generateTensorFromValues('int32', [unetBatch, textEmbeddingSequenceLength], token_ids) };
        const clonedtextEncoderInputs = Utils.clone(textEncoderInputs);
        const textEncoderOutputs = await textEncoderSession.run(clonedtextEncoderInputs);

        const encoder_hidden_states = Utils.generateTensorFromBytes('float16', [unetBatch, textEmbeddingSequenceLength, textEmbeddingSequenceWidth], textEncoderOutputs['last_hidden_state'].data);
        unetInputs['encoder_hidden_states'] = encoder_hidden_states;
        let initialSigma = defaultSigmas[0];

        const latentsTensor = unetInputs['sample'];
        const latentElementCount = latentsTensor.size;
        const halfLatentElementCount = latentElementCount / 2; // Given [2, 4, 64, 64], we want only the first batch.
        let latents = await latentsTensor.getData();
        let halfLatents = latents.subarray(0, halfLatentElementCount); // First batch only.
        prescaleLatentSpace(/*inout*/ halfLatents, initialSigma);

        Utils.log('Beginning Unet loop execution...');

        let timeSteps = new BigInt64Array(unetInputs['timestep'].cpuData.buffer);

        // Repeat unet detection and denosing until convergence (typically 25 iterations).
        for (var i = 0; i < unetIterationCount; ++i) {
            // Update time step.
            let timeStepValue = defaultTimeSteps[i];
            timeStepValue = BigInt(Math.round(timeStepValue)); // Round, because this ridiculous language throws an exception otherwise.
            timeSteps[0] = timeStepValue;
            timeSteps[1] = timeStepValue;

            // Clone the inputs because ORT unfortunately steals the tensor data in the worker thread.
            const clonedInputs = Utils.clone(unetInputs);

            // Prescale the latent values.
            // Copy first batch to second batch, duplicating latents for positive and negative prompts.
            let clonedLatents = await clonedInputs['sample'].getData(); // basically Uint16Array(clonedInputs['sample'].cpuData.buffer)
            let halfClonedLatents = clonedLatents.subarray(0, halfLatentElementCount);
            scaleLatentSpaceForPrediction(/*inout*/ halfClonedLatents, i);
            clonedLatents.copyWithin(halfLatentElementCount, 0, halfLatentElementCount); // Copy lower half to upper half.

            if (enableNanContaminationDebugging) {
                // Fill upper half of latent space with NaN's to detect any cross-batch pollution.
                for (let i = halfLatentElementCount; i < latentElementCount; ++i) {
                    clonedLatents[i] = 0x7FFF;
                }
            }

            if (enablePrintDebugging && i == 0) {
                console.log('clonedLatents:');
                console.log(clonedLatents);
            }

            const unetOutputs = await unetModelSession.run(clonedInputs);
            let predictedNoise = new Uint16Array(unetOutputs['out_sample'].cpuData.buffer);
            if (enablePrintDebugging && i == 0) {
                console.log('predictedNoise:');
                console.log(predictedNoise);
                console.log(`latents.length == ${latents.length}`);
                console.log(`latents[0] == ${latents[0]} before`);
            }

            denoiseLatentSpace(/*inout*/ latents, i, predictedNoise);
            if (enablePrintDebugging && i == 0) {
                console.log('adjusted latents:');
                console.log(latents);
            }
        }

        Utils.log('Beginning VAE decode...');

        // Decode from latent space.
        applyVaeScalingFactor(/*inout*/ halfLatents);
        let dimensions = latentsTensor.dims.slice(0);
        dimensions[0] = 1; // Set batch size to 1, ignore the 2nd batch for the negative prediction.
        vaeDecodeInputs['latent_sample'] = Utils.generateTensorFromBytes('float16', dimensions, halfLatents.slice(0));
        const clonedVaeDecodeInputs = Utils.clone(vaeDecodeInputs);
        const decodedOutputs = await vaeDecodeModelSession.run(clonedVaeDecodeInputs);
        return decodedOutputs['sample'];
    }

    async function executeStableDiffusionAndDisplayOutput() {
        try {
            displayEmptyCanvasPlaceholder();

            const executionStartTime = performance.now();
            let rgbPlanarPixels = await executeStableDiffusion();
            const executionTime = performance.now() - executionStartTime;
            Utils.log(`Total execution time: ${(executionTime / 1000).toFixed(2)}s`);

            displayPlanarRGB(await rgbPlanarPixels.getData());
        }
        catch (e) {
            console.log('Exception: ', e);
            Utils.appendStatus('Exception: ' + e);
        }
    }

    async function generateNextImage() {
        executeStableDiffusionAndDisplayOutput();
        let latentSpace = new Uint16Array(unetInputs['sample'].cpuData.buffer);
        generateNoise(/*inout*/ latentSpace, seed);
        seed++;
        startButton.removeAttribute('disabled');
    }

    displayEmptyCanvasPlaceholder();

    const executionProvider = getQueryVariable('provider', 'webnn');
    Utils.log('Execution Provider: ' + executionProvider);

    displayEmptyCanvasPlaceholder();
    initializeOnnxRuntime();
</script>

</html>